<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="h_dfHlSBtZBWZvnonl9V5A4ESCnKdWFfGXRYAwBSzyU"> <meta name="msvalidate.01" content="7CF01C322790A62837ADC6223103E73D"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AI Researchers and Social Inequality | jackjburnett </title> <meta name="author" content="Jack Burnett"> <meta name="description" content="Proposed responsible AI article submission for The Conversation"> <meta name="keywords" content="interactive-ai, hci, ai, student, phd, bristol, uob, jackburnett, jack-burnett"> <meta property="og:site_name" content="jackjburnett"> <meta property="og:type" content="website"> <meta property="og:title" content="jackjburnett | AI Researchers and Social Inequality"> <meta property="og:url" content="https://jackjburnett.github.io/projects/ethics/"> <meta property="og:description" content="Proposed responsible AI article submission for The Conversation"> <meta property="og:image" content="assets/img/prof_pic.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="AI Researchers and Social Inequality"> <meta name="twitter:description" content="Proposed responsible AI article submission for The Conversation"> <meta name="twitter:image" content="assets/img/prof_pic.png"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jack Burnett"
        },
        "url": "https://jackjburnett.github.io/projects/ethics/",
        "@type": "WebSite",
        "description": "Proposed responsible AI article submission for The Conversation",
        "headline": "AI Researchers and Social Inequality",
        
        "sameAs": ["https://orcid.org/0000-0001-8472-6121", "https://github.com/jackjburnett", "https://www.linkedin.com/in/jackjburnett", "https://research-information.bris.ac.uk/en/persons/jack-burnett"],
        
        "name": "Jack Burnett",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon-32x32.png?b2827afa11a21241440f156a0bf65f55"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jackjburnett.github.io/projects/ethics/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> jackjburnett </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">AI Researchers and Social Inequality</h1> <p class="post-description">Proposed responsible AI article submission for The Conversation</p> </header> <article> <p>PDF Version: <a href="https://jackjburnett.github.io/assets/pdf/AI_Researchers_Inequality.pdf" target="\_blank">AI Researchers and Social Inequality</a></p> <h2 id="article">Article</h2> <blockquote> <p>‘Inequality is an inevitable product of capitalist activity, and expanding equality of opportunity only increases it’ - Muller, 2013</p> </blockquote> <p>To preface this article, this will not be a direct critique of capitalism or how modern corporations generate profit; instead, this article aims to provide AI researchers with insight into the potential harms present within training data and how they can leverage their knowledge of this to work towards a more just society. As I hold left-wing political values myself, this article may contain bold statements that lean on the verge of anti-capitalist ideologies.</p> <p>Turning our attention to the subject at hand, AI’s negative impact on equality can be seen in recent articles, such as <a href="https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity" rel="external nofollow noopener" target="_blank">Kristalina Georgieva’s analysis of AI impacts on the Global Economy</a> [18] and <a href="https://www.jbs.cam.ac.uk/2023/the-dark-side-of-ai-algorithmic-bias-and-global-inequality/" rel="external nofollow noopener" target="_blank">Michael Barrett’s article on algorithmic bias</a> [4]; however, there is insufficient attention on the <a href="https://www.hlk-ip.com/the-implications-of-biased-ai-models-on-the-financial-services-industry/" rel="external nofollow noopener" target="_blank">social inequalities that can be present within datasets used to train AI models</a> [39]. AI researchers are well aware of <a href="https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai" rel="external nofollow noopener" target="_blank">biases within AI systems</a> [31], yet are also ‘largely oblivious to existing scholarship on social inequality’ [49]; how can we expect AI researchers to identify social inequalities present within data, and leverage this knowledge to <a href="https://www.umass.edu/news/article/actively-addressing-inequalities-promotes-social-change" rel="external nofollow noopener" target="_blank">promote social change</a> [42], if they are unaware of what to search for?</p> <p>In the ideal world, all AI researchers would be experts on the social issues that may affect the datasets they use and the potential socio-economic impacts of their systems; to quote Philip K. Dick <a href="https://www.goodreads.com/quotes/996658-we-do-not-have-the-ideal-world-such-as-we" rel="external nofollow noopener" target="_blank">‘we do not live in an ideal world’’’</a>. Although the notion of AI researchers being omniscient regarding social issues is unrealistic, I believe that researchers should be expected to have an awareness of the potential issues that affect the communities represented by their data and the critical thinking skills required to identify when these issues are present. A great example of this is <a href="https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/" rel="external nofollow noopener" target="_blank">‘racist predictive policing algorithms’</a> [21], where the AI system itself is deemed to be racist, so safeguards are implemented within the system; however, anyone aware of the social issues within policing would easily identify that this system is a by-product of <a href="https://www.theguardian.com/uk-news/2024/jan/05/head-of-britains-police-chiefs-says-force-is-institutionally-racist-gavin-stephens" rel="external nofollow noopener" target="_blank">institutional racism</a> [13] and is simply a reflection of the state of policing itself.</p> <p>AI systems implement safeguards to shelter users from biases present, allowing users to be blissfully ignorant of underlying issues within the datasets and models they’re interacting with; if, instead, these systems gave users a true reflection of the data and societal issues present, we would enable said users to become more <a href="https://insights.paramount.com/post/global-young-people-are-socially-aware-but-face-some-barriers-to-getting-involved/" rel="external nofollow noopener" target="_blank">socially aware</a> [26] as these issues are now extended onto them. Exposing biases to users is a double-edged sword, though, as <a href="https://www.scientificamerican.com/article/humans-absorb-bias-from-ai-and-keep-it-after-they-stop-using-the-algorithm/" rel="external nofollow noopener" target="_blank">humans absorb the biases presented to them through AI systems</a> [30]. Perhaps Disney’s approach to social issues and bias may be suitable for AI, through <a href="https://www.nytimes.com/2020/10/18/business/media/disney-plus-disclaimers.html" rel="external nofollow noopener" target="_blank">acknowledging the harmful impacts of biases and prompting users to learn from them and use them to drive conversations</a> [38]. Some may disagree with the notion of maintaining system biases as it harms system functionality [17], others may disagree as AI is an <a href="https://hbr.org/2018/02/can-we-keep-our-biases-from-creeping-into-ai" rel="external nofollow noopener" target="_blank">‘opportunity to build technology with less human bias and inequality’</a> [41]; your view on this will stem from whether you believe AI systems should reflect modern society or the ‘perfect society’, I believe in the former.</p> <p><em>Figure 1: Disney’s Stereotype Disclaimer</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/ethics/disney-480.webp 480w,/assets/img/projects/ethics/disney-800.webp 800w,/assets/img/projects/ethics/disney-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/ethics/disney.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>How bias is handled within AI systems is not the main concern when discussing AI researchers and social inequality; this article’s title suggests that AI researchers are furthering social inequality through ignorance, a claim I strongly believe. <a href="https://www.nytimes.com/2012/08/12/magazine/a-bystanders-crime.html" rel="external nofollow noopener" target="_blank">A bystander’s crime</a> [29] refers to a situation where an individual witnesses an unlawful act or harmful incident but fails to intervene or report it to the necessary authorities; in the context of discrimination crimes, the individual perpetuates existing inequalities by failing to act. I believe that AI researchers who learn of societal inequalities through data analysis or model training and then ignore them or ‘safeguard’ against them without acting to help resolve the inequalities should be held accountable. While AI researchers may not have the power to implement social policy to lessen or eradicate inequalities they identify within datasets, they do have the power to <a href="https://scholar.google.co.uk/scholar?q=report+inequality+dataset&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" rel="external nofollow noopener" target="_blank">report the inequalities</a> [20, 33], <a href="https://devinit.org/blog/why-good-data-is-key-to-unlocking-gender-equality/" rel="external nofollow noopener" target="_blank">use the data to shed light on the issues present</a> [43], <a href="https://quantilus.com/article/ais-role-in-reducing-inequalities/" rel="external nofollow noopener" target="_blank">develop tools that assist in mitigating the inequalities</a> [24], and <a href="https://www.aristotle.com/blog/2023/11/advocacy-in-the-age-of-ai-harnessing-the-future-to-amplify-your-voice/" rel="external nofollow noopener" target="_blank">advocate for social change</a> [3]; I believe this is especially true when researchers, or their employers, are profiting from the use of datasets where inequality is present, even if the inequalities themselves are not used for financial gain or only contribute to profits indirectly.</p> <p>There is a need for political change, not technical safeguards that mask society’s ills, to help promote a just society, and this is where I believe AI researchers can begin to influence change rather than perpetuate inequality. <a href="https://ideascale.com/blog/what-is-data-driven-decision-making-in-government/" rel="external nofollow noopener" target="_blank">Modern governments are shifting towards data-driven governance</a> [27], meaning governments rely on data that highlights social inequalities when creating social policies. AI researchers should have a moral duty to report datasets that contain proof of social inequalities to authorities that can influence social policy-making; however, as trade secrets are seemingly a priority over social welfare for modern corporations [14], it may be more realistic to expect researchers to leverage dataset findings for social change, rather than sharing the data itself.</p> <p>It may seem unfair to burden AI researchers with this moral duty, as most individuals entered this field with the <a href="https://www.inspiritai.com/blogs/ai-student-blog/why-i-decided-to-pursue-a-career-in-ai" rel="external nofollow noopener" target="_blank"> goal of building algorithms and AI tools for reasons outside of social equality</a> [11]; however, I would argue that having access to <a href="https://astrato.io/blog/three-times-data-changed-the-world/" rel="external nofollow noopener" target="_blank">large and potentially influential datasets</a> [5] makes the role inherently political. Arguing that an AI researcher shouldn’t have the moral duty to fight for social equality based on dataset findings, as this is not why they entered this career path, is akin to arguing that a teacher doesn’t have a moral duty to fight for child welfare as they went into teaching to enhance knowledge on a topic they’re passionate about, rather than to look after children; I believe that advocating for social equality is an integral aspect of AI research.</p> <p>While researching the topic of AI and social inequality, I could not find evidence of AI corporations that paid reparations to the communities affected by social inequalities within the datasets being used for profit. <a href="https://chatgpt.com/" rel="external nofollow noopener" target="_blank">ChatGPT</a> and <a href="https://gemini.google.com/app" rel="external nofollow noopener" target="_blank">Gemini</a> would have generated revenue from their services while the tools were outputting <a href="https://fortune.com/well/2023/10/20/chatgpt-google-bard-ai-chatbots-medical-racism-black-patients-health-care/" rel="external nofollow noopener" target="_blank">racist medical theories</a> [8] and <a href="https://equalitynow.org/news_and_insights/chatgpt-4-reinforces-sexist-stereotypes/" rel="external nofollow noopener" target="_blank">sexist stereotypes</a> [37], yet neither corporation took action to support the communities harmed within the training data from which they financially benefited. It may seem extreme to expect AI companies to pay compensation to victims of social inequality within their data; however, I believe that because corporations profit from social inequalities, they transition from bystanders to oppressors as they now benefit from these issues. AI researchers within these companies could also be viewed as oppressors due to their salaries being funded by social inequality data; however, this is an unfair claim as many researchers within these companies are not in positions of power within the corporate structure and, due to the nature of research, as many researchers engage with conferences and research projects for social good independently, such as <a href="https://warwick.ac.uk/research/data-science/warwick-data/dssgx/" rel="external nofollow noopener" target="_blank">Data Science for Social Good</a> [1].</p> <p><em>Figure 2: Example Sexist ChatGPT Response</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/ethics/chatgpt-480.webp 480w,/assets/img/projects/ethics/chatgpt-800.webp 800w,/assets/img/projects/ethics/chatgpt-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/ethics/chatgpt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The goal of this article was not to blame AI researchers for inequalities in society; instead, it was to make AI researchers, such as myself, aware of how ignoring social inequalities perpetuates them while identifying advocacy as a moral duty for AI researchers. AI research has <a href="https://www.technologyreview.com/2021/05/19/1025016/embracing-the-rapid-pace-of-ai/" rel="external nofollow noopener" target="_blank">enabled worldwide change at a rapid pace</a> [25], so it is paramount to ensure that the researchers driving the change are aware of their impact on society.</p> <h2 id="cover-letter">Cover Letter</h2> <p>Dear Editor-in-Chief, I am sending you the article ‘How AI Researchers are Furthering Social Inequality through Ignorance’ by Burnett. I would like to have the article considered for publication in The Conversation under the ‘Science + Tech’ category.</p> <p>I believe that the AI researcher community would benefit from this article as it pertains to the values-based and duties-based ethics of AI research. From a values-based standpoint, the article aims to promote the notion of equality and prioritise the reduction of inequality within research by leading researchers to make decisions that build upon these principles; this notion was furthered by implying that AI researchers have a moral duty to ensure social inequality is reduced. The article takes a Kantian approach to philosophy by identifying that AI researchers have a moral duty to promote equality regardless of personal desires. The article encourages AI researchers to find methods to use their position of power for good; however, the intentions and principles of researchers, rather than their actions, are the area of focus of the article. Ulgen [47] discusses how Responsible AI relies on human agency and developers taking responsibility for actions performed by AI systems, justifying this viewpoint through Kantian ethics. Current ethical AI discussions focus on Consequentialism [9, 16] and the moral duty of AI systems [40], which I believe detracts from the responsibilities of AI researchers; this article aims to refocus the burden of moral duties back onto AI researchers by highlighting their role within social justice.</p> <p>The article focuses on AI researchers and how they can impact the individuals and communities within their datasets. Corporations were not discussed within the article as current ethical guidelines for AI inherently focus on corporations and the systems themselves [23], as does research that discusses AI ethics [32, 48]. The article leans toward egalitarianism by identifying that promoting equality among individuals within society should be the primary concernof AI researchers. A Marxist view of trade secrets was taken within the article; this stance was justified through literature identifying the harms of trade secrets [14] rather than being explicitly stated. Equality within AI focuses onthe duty of researchers to ensure that models are trained on diverse data to limit discriminatory outcomes [15], rather than the duties of the researchers once inequality has been identified or is inevitable within the system due to societal biases. The moral duty proposed in the article is based on the concept that AI researchers are in a position of power over those within their datasets; therefore, AI researchers can be complicit in discriminatory omission by failing to act to prevent inequality when it is identified. If an individual does not hold the viewpoint of AI researchers being in a position of power, this moral duty can be justified through Kant’s deontological ethical framework [34].</p> <p>I believe that the article’s originality comes from the focus on AI researchers rather than AI companies and systems, alongside identifying moral duties for researchers rather than proposing legislation and regulation. Current articles discuss methods to handle bias within datasets [45] and the need for safeguards [10] rather than the role of researchers within society once bias has been found.</p> <p>Adib-Moghaddam’s article on biased AI algorithms [2] discusses how bad data can suppress minorities within society and the impact of social inequality on AI systems. While Adib-Moghaddam’s article [2] focuses on the need for legislation, it does identify that the first step towards legislation is an awareness of current issues; the proposed article on the moral duties of AI researchers could be viewed as the first step towards ethical and responsible AI legislation.</p> <p>Future articles could extend the moral duties to corporations and propose regulatory and political changes; however, articles of this nature will likely be lost in the sea of literature on this topic. I also believe that once corporations and regulations become the focus point, corporate interests and bureaucratic red tape will be used to ‘argue’ the feasibility of ensuring social justice for all. A concerted effort to prioritise ethical considerations and hold businesses accountable for their actions within AI is required before ethical and moral duties can be extended to corporations and formalised into regulations.</p> <p>AI bias is consistently in the news, from stereotyped outputs [46] to predictive algorithms that stigmatise individuals [36], yet discussions on how to manage bias result in either proposing legislation [12], reiterating that diverse data is required [7], or suggesting a Human-in-the-Loop approach [44]; current discussions put pressure on corporations and government rather than the workers, yet it is AI researchers who shape the current AI landscape [6, 22]. As the general public is becoming increasingly worried that AI will make daily life worse [28], AI researchers should lead by example to change public perception of AI. If AI researchers actively advocated against social inequality while producing systems that had less biased outputs, I believe that the general public’s perception of AI would become more positive. As AI researchers can improve public perception of their profession through advocacy, it could be seen as a professional duty to fight for social justice.</p> <p>While the article focuses on AI researchers, the core concept of using dataset knowledge to fight against social inequality can apply to all researchers and analysts who work with data. As an AI researcher, I find that it is easy to become desensitised to the potential impact of systems that are being developed due to the continual articles and posts about the harm of AI; I partially wrote the article to remind myself that as an AI researcher, I have the power to combat social inequality even if the systems I’m working on aren’t being developed or designed to do so. I do not believe that a single article by a PhD student will make much of an impact on the AI community as a whole, especially due to how slacktivism has promoted individuals to regurgitate articles they’ve read on LinkedIn and social media without much thought; however, I still have hope that the article can inspire other fresh researchers to become more conscious of their ability to combat social inequalities.</p> <p>I believe the proposed article sits within the ‘Human Agency’ and ‘Ethics’ areas of responsible AI [19], and that the proposed article bridges the gap between ethical AI and the moral duties of individuals within society. I hope that the proposed article can inspire further discussions on societal morality and AI, as current discussions seem to isolate AI from society.</p> <p>Thank you for considering this article for publication.</p> <h2 id="bibliography">Bibliography</h2> <p>[1] May 2024. url: <a href="https://warwick.ac.uk/research/data-science/warwick-data/dssgx/" rel="external nofollow noopener" target="_blank">https://warwick.ac.uk/research/data-science/warwick-data/dssgx/</a>. <br> [2] Arshin Adib-Moghaddam. For minorities, biased AI algorithms can damage almost every part of life. Aug. 2023. url: <a href="https://theconversation.com/for-minorities-biased-ai-algorithms-can-damage-almost-every-part-of-ife-211778" rel="external nofollow noopener" target="_blank">https://theconversation.com/for-minorities-biased-ai-algorithms-can-damage-almost-every-part-of-ife-211778</a>. <br> [3] Aristotle. Advocacy in the age of AI: Harnessing the future to amplify your voice. Mar. 2024. url: <a href="https://www.aristotle.com/blog/2023/11/advocacy-in-the-age-of-ai-harnessing-the-future-to-amplify-your-voice/" rel="external nofollow noopener" target="_blank">https://www.aristotle.com/blog/2023/11/advocacy-in-the-age-of-ai-harnessing-the-future-to-amplify-your-voice/</a>. <br> [4] Michael Barrett. The dark side of AI: algorithmic bias and global inequality. Oct. 2023. url: <a href="https://www.jbs.cam.ac.uk/2023/the-dark-side-of-ai-algorithmic-bias-and-global-inequality/" rel="external nofollow noopener" target="_blank">https://www.jbs.cam.ac.uk/2023/the-dark-side-of-ai-algorithmic-bias-and-global-inequality/</a>. <br> [5] Piers Batchelor. Three times data changed the world. July 2023. url: <a href="https://astrato.io/blog/three-times-data-changed-the-world/" rel="external nofollow noopener" target="_blank">https://astrato.io/blog/three-times-data-changed-the-world/</a>. <br> [6] Edmon Begoli and Amir Sadovnik. What can ai researchers learn from alien hunters? May 2024. url: <a href="https://spectrum.ieee.org/artificial-general-intelligence-2668132497" rel="external nofollow noopener" target="_blank">https://spectrum.ieee.org/artificial-general-intelligence-2668132497</a>. <br> [7] Phaedra Boinodiris. The importance of diversity in AI isn’t opinion, it’s math. Feb. 2024. url: <a href="https://www.ibm.com/blog/why-we-need-diverse-multidisciplinary-coes-for-model-risk/" rel="external nofollow noopener" target="_blank">https://www.ibm.com/blog/why-we-need-diverse-multidisciplinary-coes-for-model-risk/</a>. <br> [8] Garance Burke, Matt O’Brien, and The Associated Press. Bombshell Stanford study finds Chatgpt and Google’s bard answer medical questions with racist, debunked theories that harm black patients. Oct. 2023. url: <a href="https://fortune.com/well/2023/10/20/chatgpt-google-bard-ai-chatbots-medical-racism-black-patients-health-care/" rel="external nofollow noopener" target="_blank">https://fortune.com/well/2023/10/20/chatgpt-google-bard-ai-chatbots-medical-racism-black-patients-health-care/</a>. <br> [9] Dallas Card and Noah A. Smith. “On Consequentialism and Fairness”. In: Frontiers in Artificial Intelligence 3 (2020). issn: 2624-8212. doi: 10.3389/frai.2020.00034. url: <a href="https://www.frontiersin.org/articles/10.3389/frai.2020.00034" rel="external nofollow noopener" target="_blank">https://www.frontiersin.org/articles/10.3389/frai.2020.00034</a>. <br> [10] Mary Carman. Understanding AI outputs: Study shows pro-western cultural bias in the way AI decisions are explained. Apr. 2024. url: <a href="https://theconversation.com/understanding-ai-outputs-study-shows-pro-western-cultural-bias-in-the-way-ai-decisions-are-explained-227262" rel="external nofollow noopener" target="_blank">https://theconversation.com/understanding-ai-outputs-study-shows-pro-western-cultural-bias-in-the-way-ai-decisions-are-explained-227262</a>. <br> [11] Caleb Choo. Why I Decided to Pursue a Career in AI. Apr. 2024. url: <a href="https://www.inspiritai.com/blogs/ai-student-blog/why-i-decided-to-pursue-a-career-in-ai" rel="external nofollow noopener" target="_blank">https://www.inspiritai.com/blogs/ai-student-blog/why-i-decided-to-pursue-a-career-in-ai</a>. <br> [12] Philip Di Salvo and Antje Scharenberg. AI bias: The organised struggle against Automated Discrimination. Mar. 2024. url: <a href="https://theconversation.com/ai-bias-the-organised-struggle-against-automated-discrimination-223988" rel="external nofollow noopener" target="_blank">https://theconversation.com/ai-bias-the-organised-struggle-against-automated-discrimination-223988</a>. <br> [13] Vikram Dodd. “Head of Britain’s police chiefs says force ‘institutionally racist’”. In: The Guardian (Jan. 2024). url: <a href="https://www.theguardian.com/uk-news/2024/jan/05/head-of-britains-police-chiefs-says-force-is-institutionally-racist-gavin-stephens" rel="external nofollow noopener" target="_blank">https://www.theguardian.com/uk-news/2024/jan/05/head-of-britains-police-chiefs-says-force-is-institutionally-racist-gavin-stephens</a>. <br> [14] Allison Durkin et al. “Addressing the Risks That Trade Secret Protections Pose for Health and Rights”. en. In: Health Hum Rights 23.1 (June 2021), pp. 129–144. <br> [15] Omon Fagbamigbe. Embracing AI: A step forward for Diversity, equity and inclusion. Mar. 2024. url: <a href="https://www.techuk.org/resource/embracing-ai-a-step-forward-for-diversity-equity-and-inclusion.html" rel="external nofollow noopener" target="_blank">https://www.techuk.org/resource/embracing-ai-a-step-forward-for-diversity-equity-and-inclusion.html</a>. <br> [16] Josiah Della Foresta. “Consequentialism &amp; Machine Ethics: Towards a Foundational Machine Ethic to Ensure the Right Action of Artificial Moral Agents”. In: Montreal AI Ethics Institute. 2020. <br> [17] Daniel James Fuchs. “The dangers of human-like bias in machine-learning algorithms”. In: Missouri S&amp;T’s Peer to Peer 2.1 (2018), p. 1. <br> [18] Kristalina Georgieva. AI Will Transform the Global Economy. Let’s Make Sure It Benefits Humanity. Jan. 2024. url: <a href="https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity" rel="external nofollow noopener" target="_blank">https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity</a>. <br> [19] Sabrina Goellner, Marina Tropmann-Frick, and Bostjan Brumen. Responsible Artificial Intelligence: A Structured Literature Review. 2024. arXiv: 2403.06910 [cs.AI]. <br> [20] Carlos Grad ́ın. Trends in global inequality using a new integrated dataset. English. Tech. rep. 61. Helsinki, Finland. doi: https://doi.org/10.35188/UNU-WIDER/2021/999-0. [21] Will Douglas Heaven. Predictive policing algorithms are racist. They need to be dismantled. July 2020. url: <a href="https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/" rel="external nofollow noopener" target="_blank">https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/</a>. <br> [22] Will Henshall. Researchers develop new way to purge AI of Unsafe Knowledge. Mar. 2024. url: <a href="https://time.com/6878893/ai-artificial-intelligence-dangerous-knowledge/" rel="external nofollow noopener" target="_blank">https://time.com/6878893/ai-artificial-intelligence-dangerous-knowledge/</a>. <br> [23] Eleanore Hickman and Martin Petrin. “Trustworthy AI and Corporate Governance: The EU’s Ethics Guidelines for Trustworthy Artificial Intelligence from a Company Law Perspective”. In: European Business Organization Law Review 22.4 (Dec. 2021), pp. 593–625. issn: 1741-6205. doi: 10.1007/s40804-021-00224-0. url: <a href="https://doi.org/10.1007/s40804-021-00224-0" rel="external nofollow noopener" target="_blank">https://doi.org/10.1007/s40804-021-00224-0</a>. <br> [24] Quantilus Innovation. AI’s Role In Reducing Inequalities. Dec. 2022. url: <a href="https://quantilus.com/article/ais-role-in-reducing-inequalities/" rel="external nofollow noopener" target="_blank">https://quantilus.com/article/ais-role-in-reducing-inequalities/</a>. <br> [25] MIT Technology Review Insights. Embracing the rapid pace of ai. Sept. 2023. url: <a href="https://www.technologyreview.com/2021/05/19/1025016/embracing-the-rapid-pace-of-ai/" rel="external nofollow noopener" target="_blank">https://www.technologyreview.com/2021/05/19/1025016/embracing-the-rapid-pace-of-ai/</a>. <br> [26] Matthew Jafar. Global Young People Are Socially Aware, but Face Some Barriers to Getting Involved. Aug. 2019. url: <a href="https://insights.paramount.com/post/global-young-people-are-socially-aware-but-face-some-barriers-to-getting-involved/" rel="external nofollow noopener" target="_blank">https://insights.paramount.com/post/global-young-people-are-socially-aware-but-face-some-barriers-to-getting-involved/</a>. <br> [27] Nick Jain. What is Data-Driven Decision Making in Government? Definition, Implementation, Improvement, Engagement, Challenges, and Considerations. Feb. 2024. url: <a href="https://ideascale.com/blog/what-is-data-driven-decision-making-in-government/" rel="external nofollow noopener" target="_blank">https://ideascale.com/blog/what-is-data-driven-decision-making-in-government/</a>. <br> [28] Khari Johnson. People are increasingly worried AI will make daily life worse. Aug. 2023.url: <a href="https://www.wired.com/story/fast-forward-people-are-increasingly-worried-artificial-intelligence/" rel="external nofollow noopener" target="_blank">https://www.wired.com/story/fast-forward-people-are-increasingly-worried-artificial-intelligence/</a>. <br> [29] Chuck Klosterman. “A Bystander’s Crime”. In: The New York Times (Aug. 2012). url:<a href="https://www.nytimes.com/2012/08/12/magazine/a-bystanders-crime.html" rel="external nofollow noopener" target="_blank">https://www.nytimes.com/2012/08/12/magazine/a-bystanders-crime.html</a>. <br> [30] Lauren Leffer. Humans Absorb Bias from AI—And Keep It after They Stop Using the Algorithm. Oct. 2023. url: <a href="https://www.scientificamerican.com/article/humans-absorb-bias-from-ai-and-keep-it-after-they-stop-using-the-algorithm/" rel="external nofollow noopener" target="_blank">https://www.scientificamerican.com/article/humans-absorb-bias-from-ai-and-keep-it-after-they-stop-using-the-algorithm/</a>. <br> [31] James Manyika, Jake Silberg, and Brittany Presten. What Do We Do About the Biases in AI? Oct. 2019. url: <a href="https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai" rel="external nofollow noopener" target="_blank">https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai</a>. <br> [32] Anne-Sophie Mayer et al. “How corporations encourage the implementation of AI ethics”. In: Apr. 2021. <br> [33] Elena Meschi and Francesco Scervini. “A new dataset on educational inequality”. In: Empirical Economics 47.2 (Sept. 2014), pp. 695–716. issn: 1435-8921. doi: 10.1007/s00181- 013- 0758- 6. url: <a href="https://doi.org/10.1007/s00181-013-0758-6" rel="external nofollow noopener" target="_blank">https://doi.org/10.1007/s00181-013-0758-6</a>. <br> [34] David Misselbrook. “Duty, Kant, and deontology”. en. In: Br J Gen Pract 63.609 (Apr. 2013), p. 211. <br> [35] Jerry Z. Muller. “Capitalism and Inequality: What the Right and the Left Get Wrong”. In: Foreign Affairs 92.2 (2013), pp. 30–51. issn: 00157120. url: <a href="http://www.jstor.org/stable/23527455" rel="external nofollow noopener" target="_blank">http://www.jstor.org/stable/23527455</a> (visited on 04/14/2024). <br> [36] Madhumita Murgia and Helena Vieira. AI can do harm when people don’t have a voice. Apr. 2024. url: <a href="https://blogs.lse.ac.uk/businessreview/2024/03/25/madhumita-murgia-ai-can-do-harm-when-people-dont-have-a-voice/" rel="external nofollow noopener" target="_blank">https://blogs.lse.ac.uk/businessreview/2024/03/25/madhumita-murgia-ai-can-do-harm-when-people-dont-have-a-voice/</a>. <br> [37] Equality Now. CHATGPT-4 reinforces sexist stereotypes by stating a girl cannot “Handle technicalities and numbers” in engineering. Mar. 2023. url: <a href="https://equalitynow.org/newsandinsights/chatgpt-4-reinforces-sexist-stereotypes/" rel="external nofollow noopener" target="_blank">https://equalitynow.org/newsandinsights/chatgpt-4-reinforces-sexist-stereotypes/</a>. <br> [38] Bryan Pietsch. “Disney Adds Warnings for Racist Stereotypes to Some Older Films”. In: The New York Times (Oct. 2020). url: <a href="https://www.nytimes.com/2020/10/18/business/media/disney-plus-disclaimers.html" rel="external nofollow noopener" target="_blank">https://www.nytimes.com/2020/10/18/business/media/disney-plus-disclaimers.html</a>. <br> [39] Jamie Rowlands. The implications of biased AI models on the Financial Services Industry. Nov. 2023. url: <a href="https://www.hlk-ip.com/the-implications-of-biased-ai-models-on-the-financial-services-industry/" rel="external nofollow noopener" target="_blank">https://www.hlk-ip.com/the-implications-of-biased-ai-models-on-the-financial-services-industry/</a>. <br> [40] Jeff Sebo and Robert Long. “Moral consideration for AI systems by 2030”. In: AI and Ethics (Dec. 2023). issn: 2730-5961. doi: 10.1007/s43681-023-00379-1. url: <a href="https://doi.org/10.1007/s43681-023-00379-1" rel="external nofollow noopener" target="_blank">https://doi.org/10.1007/s43681-023-00379-1</a>. <br> [41] Kriti Sharma. Can We Keep Our Biases from Creeping into AI? Feb. 2018. url: <a href="https://hbr.org/2018/02/can-we-keep-our-biases-from-creeping-into-ai" rel="external nofollow noopener" target="_blank">https://hbr.org/2018/02/can-we-keep-our-biases-from-creeping-into-ai</a>. <br> [42] Patty Shillington. Actively Addressing Inequalities Promotes Social Change. June 2021. url: <a href="https://www.umass.edu/news/article/actively-addressing-inequalities-promotes-social-change" rel="external nofollow noopener" target="_blank">https://www.umass.edu/news/article/actively-addressing-inequalities-promotes-social-change</a>. <br> [43] Fionna Smyth. Why good data is key to unlocking gender equality. Mar. 2023. url: 2024. <a href="https://devinit.org/blog/why-good-data-is-key-to-unlocking-gender-equality/" rel="external nofollow noopener" target="_blank">https://devinit.org/blog/why-good-data-is-key-to-unlocking-gender-equality/</a>. <br> [44] Solange Sobral. Ai and the need for human oversight. Nov. 2023. url: <a href="https://www.business-reporter.co.uk/ai--automation/ai-and-the-need-for-human-oversight" rel="external nofollow noopener" target="_blank">https://www.business-reporter.co.uk/ai–automation/ai-and-the-need-for-human-oversight</a>. <br> [45] T.J. Thomson and Ryan J. Thomas. Ageism, sexism, classism and more: 7 examples of bias in AI-generated images. Mar. 2024. url: <a href="https://theconversation.com/ageism-sexism-classism-and-more-7-examples-of-bias-in-ai-generated-images-208748" rel="external nofollow noopener" target="_blank">https://theconversation.com/ageism-sexism-classism-and-more-7-examples-of-bias-in-ai-generated-images-208748</a>. <br> [46] Nitasha Tiku, Kevin Schaul, and Szu Yu Chen. Ai generated images are biased, showing the world through stereotypes. Nov. 2023. url: <a href="https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/" rel="external nofollow noopener" target="_blank">https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/</a>. <br> [47] Ozlem Ulgen. “Kantian Ethics in the Age of Artificial Intelligence and Robotics (2017) 43 QIL, Zoom-in (Questions of International Law/Question de Droit International/Questioni di Diritto) 59-83”. In: 43 (Jan. 2017), pp. 59–83. <br> [48] Darrell M. West et al. The role of corporations in addressing AI’s ethical dilemmas. Mar. 2022. url: <a href="https://www.brookings.edu/articles/how-to-address-ai-ethical-dilemmas/" rel="external nofollow noopener" target="_blank">https://www.brookings.edu/articles/how-to-address-ai-ethical-dilemmas/</a>. <br> [49] Mike Zajko. “Artificial intelligence, algorithms, and social inequality: Sociological contributions to contemporary debates”. In: Sociology Compass 16.3 (2022), e12962. doi: https://doi.org/10.1111/soc4.12962. eprint: https://compass.onlinelibrary.wiley.com/doi/pdf/10.1111/soc4.12962. url: <a href="https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/soc412962" rel="external nofollow noopener" target="_blank">https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/soc412962</a>.</p> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jack Burnett. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 08, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>